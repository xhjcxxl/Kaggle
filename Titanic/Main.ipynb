{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先获取数据，然后使用 神经网络 来进行逻辑回归分类，这个比较好算一些\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# 随机森林 补全数据\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# label Onehot encode preprocessing 预处理\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 归一化\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891,)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "# get Y\n",
    "train_Y = train_data['Survived'].as_matrix()\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of      PassengerId  Pclass                                               Name  \\\n",
       "0            892       3                                   Kelly, Mr. James   \n",
       "1            893       3                   Wilkes, Mrs. James (Ellen Needs)   \n",
       "2            894       2                          Myles, Mr. Thomas Francis   \n",
       "3            895       3                                   Wirz, Mr. Albert   \n",
       "4            896       3       Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
       "5            897       3                         Svensson, Mr. Johan Cervin   \n",
       "6            898       3                               Connolly, Miss. Kate   \n",
       "7            899       2                       Caldwell, Mr. Albert Francis   \n",
       "8            900       3          Abrahim, Mrs. Joseph (Sophie Halaut Easu)   \n",
       "9            901       3                            Davies, Mr. John Samuel   \n",
       "10           902       3                                   Ilieff, Mr. Ylio   \n",
       "11           903       1                         Jones, Mr. Charles Cresson   \n",
       "12           904       1      Snyder, Mrs. John Pillsbury (Nelle Stevenson)   \n",
       "13           905       2                               Howard, Mr. Benjamin   \n",
       "14           906       1  Chaffee, Mrs. Herbert Fuller (Carrie Constance...   \n",
       "15           907       2      del Carlo, Mrs. Sebastiano (Argenia Genovesi)   \n",
       "16           908       2                                  Keane, Mr. Daniel   \n",
       "17           909       3                                  Assaf, Mr. Gerios   \n",
       "18           910       3                       Ilmakangas, Miss. Ida Livija   \n",
       "19           911       3              Assaf Khalil, Mrs. Mariana (Miriam\")\"   \n",
       "20           912       1                             Rothschild, Mr. Martin   \n",
       "21           913       3                          Olsen, Master. Artur Karl   \n",
       "22           914       1               Flegenheim, Mrs. Alfred (Antoinette)   \n",
       "23           915       1                    Williams, Mr. Richard Norris II   \n",
       "24           916       1    Ryerson, Mrs. Arthur Larned (Emily Maria Borie)   \n",
       "25           917       3                            Robins, Mr. Alexander A   \n",
       "26           918       1                       Ostby, Miss. Helene Ragnhild   \n",
       "27           919       3                                  Daher, Mr. Shedid   \n",
       "28           920       1                            Brady, Mr. John Bertram   \n",
       "29           921       3                                  Samaan, Mr. Elias   \n",
       "..           ...     ...                                                ...   \n",
       "388         1280       3                               Canavan, Mr. Patrick   \n",
       "389         1281       3                        Palsson, Master. Paul Folke   \n",
       "390         1282       1                         Payne, Mr. Vivian Ponsonby   \n",
       "391         1283       1     Lines, Mrs. Ernest H (Elizabeth Lindsey James)   \n",
       "392         1284       3                      Abbott, Master. Eugene Joseph   \n",
       "393         1285       2                               Gilbert, Mr. William   \n",
       "394         1286       3                           Kink-Heilmann, Mr. Anton   \n",
       "395         1287       1     Smith, Mrs. Lucien Philip (Mary Eloise Hughes)   \n",
       "396         1288       3                               Colbert, Mr. Patrick   \n",
       "397         1289       1  Frolicher-Stehli, Mrs. Maxmillian (Margaretha ...   \n",
       "398         1290       3                     Larsson-Rondberg, Mr. Edvard A   \n",
       "399         1291       3                           Conlon, Mr. Thomas Henry   \n",
       "400         1292       1                            Bonnell, Miss. Caroline   \n",
       "401         1293       2                                    Gale, Mr. Harry   \n",
       "402         1294       1                     Gibson, Miss. Dorothy Winifred   \n",
       "403         1295       1                             Carrau, Mr. Jose Pedro   \n",
       "404         1296       1                       Frauenthal, Mr. Isaac Gerald   \n",
       "405         1297       2       Nourney, Mr. Alfred (Baron von Drachstedt\")\"   \n",
       "406         1298       2                          Ware, Mr. William Jeffery   \n",
       "407         1299       1                         Widener, Mr. George Dunton   \n",
       "408         1300       3                    Riordan, Miss. Johanna Hannah\"\"   \n",
       "409         1301       3                          Peacock, Miss. Treasteall   \n",
       "410         1302       3                             Naughton, Miss. Hannah   \n",
       "411         1303       1    Minahan, Mrs. William Edward (Lillian E Thorpe)   \n",
       "412         1304       3                     Henriksson, Miss. Jenny Lovisa   \n",
       "413         1305       3                                 Spector, Mr. Woolf   \n",
       "414         1306       1                       Oliva y Ocana, Dona. Fermina   \n",
       "415         1307       3                       Saether, Mr. Simon Sivertsen   \n",
       "416         1308       3                                Ware, Mr. Frederick   \n",
       "417         1309       3                           Peter, Master. Michael J   \n",
       "\n",
       "        Sex   Age  SibSp  Parch              Ticket      Fare  \\\n",
       "0      male  34.5      0      0              330911    7.8292   \n",
       "1    female  47.0      1      0              363272    7.0000   \n",
       "2      male  62.0      0      0              240276    9.6875   \n",
       "3      male  27.0      0      0              315154    8.6625   \n",
       "4    female  22.0      1      1             3101298   12.2875   \n",
       "5      male  14.0      0      0                7538    9.2250   \n",
       "6    female  30.0      0      0              330972    7.6292   \n",
       "7      male  26.0      1      1              248738   29.0000   \n",
       "8    female  18.0      0      0                2657    7.2292   \n",
       "9      male  21.0      2      0           A/4 48871   24.1500   \n",
       "10     male   NaN      0      0              349220    7.8958   \n",
       "11     male  46.0      0      0                 694   26.0000   \n",
       "12   female  23.0      1      0               21228   82.2667   \n",
       "13     male  63.0      1      0               24065   26.0000   \n",
       "14   female  47.0      1      0         W.E.P. 5734   61.1750   \n",
       "15   female  24.0      1      0       SC/PARIS 2167   27.7208   \n",
       "16     male  35.0      0      0              233734   12.3500   \n",
       "17     male  21.0      0      0                2692    7.2250   \n",
       "18   female  27.0      1      0    STON/O2. 3101270    7.9250   \n",
       "19   female  45.0      0      0                2696    7.2250   \n",
       "20     male  55.0      1      0            PC 17603   59.4000   \n",
       "21     male   9.0      0      1             C 17368    3.1708   \n",
       "22   female   NaN      0      0            PC 17598   31.6833   \n",
       "23     male  21.0      0      1            PC 17597   61.3792   \n",
       "24   female  48.0      1      3            PC 17608  262.3750   \n",
       "25     male  50.0      1      0           A/5. 3337   14.5000   \n",
       "26   female  22.0      0      1              113509   61.9792   \n",
       "27     male  22.5      0      0                2698    7.2250   \n",
       "28     male  41.0      0      0              113054   30.5000   \n",
       "29     male   NaN      2      0                2662   21.6792   \n",
       "..      ...   ...    ...    ...                 ...       ...   \n",
       "388    male  21.0      0      0              364858    7.7500   \n",
       "389    male   6.0      3      1              349909   21.0750   \n",
       "390    male  23.0      0      0               12749   93.5000   \n",
       "391  female  51.0      0      1            PC 17592   39.4000   \n",
       "392    male  13.0      0      2           C.A. 2673   20.2500   \n",
       "393    male  47.0      0      0          C.A. 30769   10.5000   \n",
       "394    male  29.0      3      1              315153   22.0250   \n",
       "395  female  18.0      1      0               13695   60.0000   \n",
       "396    male  24.0      0      0              371109    7.2500   \n",
       "397  female  48.0      1      1               13567   79.2000   \n",
       "398    male  22.0      0      0              347065    7.7750   \n",
       "399    male  31.0      0      0               21332    7.7333   \n",
       "400  female  30.0      0      0               36928  164.8667   \n",
       "401    male  38.0      1      0               28664   21.0000   \n",
       "402  female  22.0      0      1              112378   59.4000   \n",
       "403    male  17.0      0      0              113059   47.1000   \n",
       "404    male  43.0      1      0               17765   27.7208   \n",
       "405    male  20.0      0      0       SC/PARIS 2166   13.8625   \n",
       "406    male  23.0      1      0               28666   10.5000   \n",
       "407    male  50.0      1      1              113503  211.5000   \n",
       "408  female   NaN      0      0              334915    7.7208   \n",
       "409  female   3.0      1      1  SOTON/O.Q. 3101315   13.7750   \n",
       "410  female   NaN      0      0              365237    7.7500   \n",
       "411  female  37.0      1      0               19928   90.0000   \n",
       "412  female  28.0      0      0              347086    7.7750   \n",
       "413    male   NaN      0      0           A.5. 3236    8.0500   \n",
       "414  female  39.0      0      0            PC 17758  108.9000   \n",
       "415    male  38.5      0      0  SOTON/O.Q. 3101262    7.2500   \n",
       "416    male   NaN      0      0              359309    8.0500   \n",
       "417    male   NaN      1      1                2668   22.3583   \n",
       "\n",
       "               Cabin Embarked  \n",
       "0                NaN        Q  \n",
       "1                NaN        S  \n",
       "2                NaN        Q  \n",
       "3                NaN        S  \n",
       "4                NaN        S  \n",
       "5                NaN        S  \n",
       "6                NaN        Q  \n",
       "7                NaN        S  \n",
       "8                NaN        C  \n",
       "9                NaN        S  \n",
       "10               NaN        S  \n",
       "11               NaN        S  \n",
       "12               B45        S  \n",
       "13               NaN        S  \n",
       "14               E31        S  \n",
       "15               NaN        C  \n",
       "16               NaN        Q  \n",
       "17               NaN        C  \n",
       "18               NaN        S  \n",
       "19               NaN        C  \n",
       "20               NaN        C  \n",
       "21               NaN        S  \n",
       "22               NaN        S  \n",
       "23               NaN        C  \n",
       "24   B57 B59 B63 B66        C  \n",
       "25               NaN        S  \n",
       "26               B36        C  \n",
       "27               NaN        C  \n",
       "28               A21        S  \n",
       "29               NaN        C  \n",
       "..               ...      ...  \n",
       "388              NaN        Q  \n",
       "389              NaN        S  \n",
       "390              B24        S  \n",
       "391              D28        S  \n",
       "392              NaN        S  \n",
       "393              NaN        S  \n",
       "394              NaN        S  \n",
       "395              C31        S  \n",
       "396              NaN        Q  \n",
       "397              B41        C  \n",
       "398              NaN        S  \n",
       "399              NaN        Q  \n",
       "400               C7        S  \n",
       "401              NaN        S  \n",
       "402              NaN        C  \n",
       "403              NaN        S  \n",
       "404              D40        C  \n",
       "405              D38        C  \n",
       "406              NaN        S  \n",
       "407              C80        C  \n",
       "408              NaN        Q  \n",
       "409              NaN        S  \n",
       "410              NaN        Q  \n",
       "411              C78        Q  \n",
       "412              NaN        S  \n",
       "413              NaN        S  \n",
       "414             C105        C  \n",
       "415              NaN        S  \n",
       "416              NaN        S  \n",
       "417              NaN        C  \n",
       "\n",
       "[418 rows x 11 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ————————————————————————————————————\n",
    "###  拿到数据先做数据分析，别急着写模型，先做好数据分析，知道哪些特征是需要的，哪些是不需要的\n",
    "\n",
    "\n",
    "# 1.数据分析 各个属性的关联状况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 3, ..., 7.25, nan, 'S'],\n",
       "       [2, 1, 1, ..., 71.2833, 'C85', 'C'],\n",
       "       [3, 1, 3, ..., 7.925, nan, 'S'],\n",
       "       ..., \n",
       "       [889, 0, 3, ..., 23.45, nan, 'S'],\n",
       "       [890, 1, 1, ..., 30.0, 'C148', 'C'],\n",
       "       [891, 0, 3, ..., 7.75, nan, 'Q']], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 每一列功能，类型\n",
    "train_data.info()\n",
    "# 获取 索引\n",
    "train_data.index\n",
    "# 获取 数据的值\n",
    "train_data.values\n",
    "\n",
    "# train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex     Pclass\n",
      "female  1         0.968085\n",
      "        2         0.921053\n",
      "        3         0.500000\n",
      "male    1         0.368852\n",
      "        2         0.157407\n",
      "        3         0.135447\n",
      "Name: Survived, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'1 Survived ------ 0 not Survived')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d2c4ca7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d2c451940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 分组 按照 给定的 标签进行分组\n",
    "gender = train_data.groupby(['Sex', 'Pclass'])\n",
    "# print(\"SUM: \", gender.sum())\n",
    "# 计算 分组 结果\n",
    "gender.count()\n",
    "# 计算 分组 求和 gender.sum()\n",
    "\n",
    "# 获取 性别 等级  生还 的关系\n",
    "Rate = (gender.sum() / gender.count())['Survived']\n",
    "print(Rate)\n",
    "plt.figure()\n",
    "Rate.plot(kind = 'bar')\n",
    "\n",
    "# 生还 状况\n",
    "plt.figure()\n",
    "train_data.Survived.value_counts().plot(kind = 'bar')\n",
    "plt.title(\"1 Survived ------ 0 not Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7d29bb21d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d29d67518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 图像 分析\n",
    "fig = plt.figure()\n",
    "fig.set(alpha = 0.2)\n",
    "#   控制图片      横纵比例  位置\n",
    "plt.subplot(231)\n",
    "train_data.Survived.value_counts().plot(kind = 'bar')\n",
    "plt.title(\"State(1 is Survived)\")\n",
    "plt.ylabel(\"people\")\n",
    "\n",
    "plt.subplot(232)\n",
    "train_data.Pclass.value_counts().plot(kind = 'bar')\n",
    "plt.title(\"Pclass(1 2 3  class)\")\n",
    "plt.ylabel(\"people\")\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.scatter(train_data.Survived, train_data.Age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论\n",
    "\n",
    "## （1）女性的存活率普遍高于男性；\n",
    "## （2）性别相同的话，舱位越好，存活率越高；\n",
    "## （3）小孩子的存活率要略高于其他年龄段，老年人不太好判断；\n",
    "## （4）有1~3个亲属的人存活率要普遍高于其他区段；\n",
    "## （5）不同地点上船的存活率略有区别，瑟堡（C）要高于另外两个地点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数据的初步处理 补全，去掉多余数据，分析出需要的特征出来\n",
    "## -----------------------------------------------------------------------------------------------------------------------------------\n",
    "## 缺值的样本占总数的比例太大，那么就可以放弃这个特征了\n",
    "## 缺值的样本适中，而且值为非连续性特征属性，那么就可以将缺值数据单独设置一个值\n",
    "## 缺值的样本适中，而且值为连续性特征属性，那么就可以将数据离散化\n",
    "## 缺值的样本很少，那么就可以拟合数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤\n",
    "- 1 剔除PassengerId，Ticket这两个个变量，我们不用。\n",
    "\n",
    "- 2 将Embarked变量补全，然后对Survived，Name，Sex， Embarked进行one-hot编码。\n",
    "\n",
    "- 3对Pclass，Fare,Sibsp和Parch进行归一化处理。\n",
    "\n",
    "- 3 根据Name，Sex，SibSp，Parch预测age将其补全。\n",
    "\n",
    "- 4 对age进行归一化处理。\n",
    "\n",
    "- 5 将未编码的Survived提出当做目标变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 剔除变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剔除 变量 PassengerId ticket\n",
    "def data_drop(X):\n",
    "    X = X.drop([\"PassengerId\", \"Ticket\"], axis = 1)\n",
    "    # X.info()\n",
    "    X.loc[X.Embarked.isnull(), \"Embarked\"] = 'S'\n",
    "    X.info()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据 编码 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name 字符串处理\n",
    "def data_Replce_name(x):\n",
    "    if 'Mrs' in x:\n",
    "        return 'Mrs'\n",
    "    elif 'Mr' in x:\n",
    "        return 'Mr'\n",
    "    else:\n",
    "        return 'Miss'\n",
    "\n",
    "# Sex 的数据处理\n",
    "def data_Encoder(X):\n",
    "    # 需要匹配的标签，下面需要转化的数据 只能在标签中，不能是其他的（就是说要么是male 要么female，没有其他的）\n",
    "    sex_fit_label = LabelEncoder().fit(X['Sex'])  # print(sex_fit_label.classes_)\n",
    "    Sex_label = sex_fit_label.transform(X['Sex']) # 将所有数据 根据标签 转化为 数字\n",
    "    Sex_label = Sex_label.reshape(-1, 1)          # 重构 维度 # print(Sex_label)\n",
    "\n",
    "    sex_fit_one = OneHotEncoder(sparse = False).fit(Sex_label) # 匹配 需要统计的数据\n",
    "    # print(\"classes: \", sex_fit_lne.n_values_) # 表示各个属性能取的值的数目 比如这里能取 0 1 那么个数就是 2 转化就是根据这个来转化的\n",
    "    Sex_one = sex_fit_one.transform(Sex_label) # 转化为 向量数据\n",
    "    # print(\"Sex_one: \\n\", Sex_one, \"\\n\")\n",
    "\n",
    "\n",
    "    # Embarked-登船港口 的数据处理\n",
    "    embarked_fit_label = LabelEncoder().fit(X['Embarked'])\n",
    "    # print(\"classes: \", embarked_fit_label.classes_)\n",
    "    Embarked_label = embarked_fit_label.transform(X['Embarked']).reshape(-1, 1) # 可以合并到上一个数据 直接进行重构\n",
    "    # print(Embarked_label)\n",
    "\n",
    "    embarked_fit_one = OneHotEncoder(sparse = False).fit(Embarked_label)\n",
    "    Embarked_one = embarked_fit_one.transform(Embarked_label)\n",
    "    # print(\"Embarked_one: \\n\", Embarked_one, \"\\n\")\n",
    "\n",
    "    # name 处理\n",
    "    X['Name'] = X['Name'].map(lambda x: data_Replce_name(x))\n",
    "    # print(X['Name'])\n",
    "    name_fit_label = LabelEncoder().fit(X['Name'])\n",
    "    # print(name_fit_label.classes_)\n",
    "    Name_label = name_fit_label.transform(X['Name']).reshape(-1, 1)\n",
    "    # print(Name_label)\n",
    "\n",
    "    name_fit_one = OneHotEncoder(sparse = False).fit(Name_label)\n",
    "    # print(name_fit_one.n_values_)\n",
    "    Name_one = name_fit_one.transform(Name_label)\n",
    "    # print(\"Name_one: \\n\", Name_one, \"\\n\")\n",
    "\n",
    "    X['Sex_0'] = Sex_one[:, 0]\n",
    "    X['Sex_1'] = Sex_one[:, 1]\n",
    "\n",
    "    X['Embarked_0'] = Embarked_one[:, 0]\n",
    "    X['Embarked_1'] = Embarked_one[:, 1]\n",
    "    X['Embarked_2'] = Embarked_one[:, 2]\n",
    "\n",
    "    X['Name_0'] = Name_one[:, 0]\n",
    "    X['Name_1'] = Name_one[:, 1]\n",
    "    X['Name_2'] = Name_one[:, 2]\n",
    "    # X.info()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass 归一化\n",
    "def data_StandardScaler(X):\n",
    "    print(X['Pclass'].shape) # 这个维度 有问题 都不是向量维度 所以下面需要进行修改\n",
    "    Pclass_scale = StandardScaler().fit(X['Pclass'].values.reshape(-1, 1)) # 记得重构 维度\n",
    "    print(Pclass_scale.mean_)\n",
    "    X['Pclass_scaled'] = Pclass_scale.transform(X['Pclass'].values.reshape(-1, 1))\n",
    "    # print(X['Pclass_scaled'])\n",
    "\n",
    "    # Fare 归一化\n",
    "    Fare_scale = StandardScaler().fit(X['Fare'].values.reshape(-1, 1))\n",
    "    print(Fare_scale.mean_)\n",
    "    X['Fare_scaled'] = Fare_scale.transform(X['Fare'].values.reshape(-1, 1))\n",
    "    # print(X['Fare_scaled'])\n",
    "\n",
    "    # SibSp 归一化\n",
    "    SibSp_scale = StandardScaler().fit(X['SibSp'].values.reshape(-1, 1))\n",
    "    print(SibSp_scale.mean_)\n",
    "    X['SibSp_scaled'] = SibSp_scale.transform(X['SibSp'].values.reshape(-1, 1))\n",
    "    # print(X['SibSp_scaled'])\n",
    "\n",
    "    Parch_scale = StandardScaler().fit(X['Parch'].values.reshape(-1, 1))\n",
    "    print(Parch_scale.mean_)\n",
    "    X['Parch_scaled'] = Parch_scale.transform(X['Parch'].values.reshape(-1, 1))\n",
    "    # print(X['Parch_scaled'])\n",
    "    X.info()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测年纪 并补全"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_missing_age(data):\n",
    "    train = data[['Age', 'SibSp_scaled', 'Parch_scaled', 'Name_0',\n",
    "                  'Name_1', 'Name_2', 'Sex_0', 'Sex_1']]\n",
    "    known_age = train[train.Age.notnull()].as_matrix()\n",
    "    if train[train.Age.isnull()].empty:\n",
    "        print(\"this is empty\")\n",
    "        rf = None\n",
    "        return data, rf\n",
    "    unknown_age = train[train.Age.isnull()].as_matrix()\n",
    "    # print(\"unknown_age:  \\n\", unknown_age)\n",
    "    print(known_age)\n",
    "    y = known_age[:, 0]\n",
    "    x = known_age[:, 1:]\n",
    "    rf = RandomForestRegressor(random_state = 0, n_estimators = 2000, n_jobs = -1)\n",
    "    rf.fit(x, y)\n",
    "    # print(rf.score(x, y))\n",
    "    predictedAges = rf.predict(unknown_age[:, 1::])\n",
    "    data.loc[data.Age.isnull(), 'Age'] = predictedAges\n",
    "    return data, rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_Processing(X):\n",
    "    X = data_drop(X)\n",
    "    X1 = data_Encoder(X)\n",
    "    X2 = data_StandardScaler(X1)\n",
    "    data_X, rf = set_missing_age(X2)\n",
    "    \n",
    "    Age_scale = StandardScaler().fit(data_X['Age'].reshape(-1, 1))\n",
    "    data_X['Age_scaled'] = Age_scale.transform(data_X['Age'].reshape(-1, 1))\n",
    "    train_X = data_X[['Sex_0', 'Sex_1', 'Embarked_0', 'Embarked_1', 'Embarked_2', 'Name_0',\n",
    "                'Name_1', 'Name_2', 'Pclass_scaled', 'Age_scaled', 'Fare_scaled']].as_matrix()\n",
    "    return train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 10 columns):\n",
      "Survived    891 non-null int64\n",
      "Pclass      891 non-null int64\n",
      "Name        891 non-null object\n",
      "Sex         891 non-null object\n",
      "Age         714 non-null float64\n",
      "SibSp       891 non-null int64\n",
      "Parch       891 non-null int64\n",
      "Fare        891 non-null float64\n",
      "Cabin       204 non-null object\n",
      "Embarked    891 non-null object\n",
      "dtypes: float64(2), int64(4), object(4)\n",
      "memory usage: 69.7+ KB\n",
      "(891,)\n",
      "[ 2.30864198]\n",
      "[ 32.20420797]\n",
      "[ 0.52300786]\n",
      "[ 0.38159371]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 22 columns):\n",
      "Survived         891 non-null int64\n",
      "Pclass           891 non-null int64\n",
      "Name             891 non-null object\n",
      "Sex              891 non-null object\n",
      "Age              714 non-null float64\n",
      "SibSp            891 non-null int64\n",
      "Parch            891 non-null int64\n",
      "Fare             891 non-null float64\n",
      "Cabin            204 non-null object\n",
      "Embarked         891 non-null object\n",
      "Sex_0            891 non-null float64\n",
      "Sex_1            891 non-null float64\n",
      "Embarked_0       891 non-null float64\n",
      "Embarked_1       891 non-null float64\n",
      "Embarked_2       891 non-null float64\n",
      "Name_0           891 non-null float64\n",
      "Name_1           891 non-null float64\n",
      "Name_2           891 non-null float64\n",
      "Pclass_scaled    891 non-null float64\n",
      "Fare_scaled      891 non-null float64\n",
      "SibSp_scaled     891 non-null float64\n",
      "Parch_scaled     891 non-null float64\n",
      "dtypes: float64(14), int64(4), object(4)\n",
      "memory usage: 153.2+ KB\n",
      "[[ 22.           0.43279337  -0.47367361 ...,   0.           0.           1.        ]\n",
      " [ 38.           0.43279337  -0.47367361 ...,   1.           1.           0.        ]\n",
      " [ 26.          -0.4745452   -0.47367361 ...,   0.           1.           0.        ]\n",
      " ..., \n",
      " [ 19.          -0.4745452   -0.47367361 ...,   0.           1.           0.        ]\n",
      " [ 26.          -0.4745452   -0.47367361 ...,   0.           0.           1.        ]\n",
      " [ 32.          -0.4745452   -0.47367361 ...,   0.           0.           1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  import sys\n",
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 9 columns):\n",
      "Pclass      418 non-null int64\n",
      "Name        418 non-null object\n",
      "Sex         418 non-null object\n",
      "Age         332 non-null float64\n",
      "SibSp       418 non-null int64\n",
      "Parch       418 non-null int64\n",
      "Fare        418 non-null float64\n",
      "Cabin       91 non-null object\n",
      "Embarked    418 non-null object\n",
      "dtypes: float64(2), int64(3), object(4)\n",
      "memory usage: 29.5+ KB\n",
      "(418,)\n",
      "[ 2.26555024]\n",
      "[ 35.54195598]\n",
      "[ 0.44736842]\n",
      "[ 0.3923445]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 21 columns):\n",
      "Pclass           418 non-null int64\n",
      "Name             418 non-null object\n",
      "Sex              418 non-null object\n",
      "Age              332 non-null float64\n",
      "SibSp            418 non-null int64\n",
      "Parch            418 non-null int64\n",
      "Fare             418 non-null float64\n",
      "Cabin            91 non-null object\n",
      "Embarked         418 non-null object\n",
      "Sex_0            418 non-null float64\n",
      "Sex_1            418 non-null float64\n",
      "Embarked_0       418 non-null float64\n",
      "Embarked_1       418 non-null float64\n",
      "Embarked_2       418 non-null float64\n",
      "Name_0           418 non-null float64\n",
      "Name_1           418 non-null float64\n",
      "Name_2           418 non-null float64\n",
      "Pclass_scaled    418 non-null float64\n",
      "Fare_scaled      418 non-null float64\n",
      "SibSp_scaled     418 non-null float64\n",
      "Parch_scaled     418 non-null float64\n",
      "dtypes: float64(14), int64(3), object(4)\n",
      "memory usage: 68.7+ KB\n",
      "[[ 34.5         -0.49947002  -0.4002477  ...,   0.           0.           1.        ]\n",
      " [ 47.           0.61699237  -0.4002477  ...,   1.           1.           0.        ]\n",
      " [ 62.          -0.49947002  -0.4002477  ...,   0.           0.           1.        ]\n",
      " ..., \n",
      " [ 28.          -0.49947002  -0.4002477  ...,   0.           1.           0.        ]\n",
      " [ 39.          -0.49947002  -0.4002477  ...,   0.           1.           0.        ]\n",
      " [ 38.5         -0.49947002  -0.4002477  ...,   0.           0.           1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  import sys\n",
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_X = data_Processing(train_data)\n",
    "\n",
    "test_data.loc[ (test_data.Fare.isnull()), 'Fare' ] = 0\n",
    "test_X = data_Processing(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ————————————————————————————————————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立 逻辑回归模型\n",
    "\n",
    "## 使用参数为：\n",
    "\n",
    "## train_X train_Y test_X test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 891)\n",
      "(1, 891)\n",
      "(11, 418)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_X.T\n",
    "train_Y = train_Y.reshape(-1, 1).T\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "test_X = test_X.T\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"train_X.csv\", train_X)\n",
    "np.savetxt(\"train_Y.csv\", train_Y)\n",
    "np.savetxt(\"test_X.csv\", test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉验证数据集\n",
    "#td_X, cv_X, td_Y, cv_Y = cross_validation.train_test_split(train_X.T,\n",
    "#                                                           train_Y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(X_row, X_col):\n",
    "    W = np.random.rand(X_row, 1) * 0.01\n",
    "    b = 0\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(W, b, train_X, Y):\n",
    "    m = train_X.shape[1]\n",
    "    Z = np.dot(W.T, train_X) + b\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    cost = (-1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    dZ = A - Y\n",
    "    dW = (1 / m) * np.dot(train_X, dZ.T)\n",
    "    db = (1 / m) * np.sum(dZ)\n",
    "    \n",
    "    assert(dW.shape == W.shape)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dW\": dW,\n",
    "             \"db\": db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Optimize(W, b, train_X, Y, number_iterations, learning_rate, print_cost = False):\n",
    "    costs = []\n",
    "    for i in range(number_iterations):\n",
    "        grads, cost = propagate(W, b, train_X, Y)\n",
    "        dW = grads[\"dW\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        W = W - learning_rate * dW\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print(\"Cost after iteration %i: %f\"%(i, cost))\n",
    "    parameters = {\"W\": W,\n",
    "                  \"b\": b}\n",
    "    grads = {\"dW\": dW,\n",
    "             \"db\": db}\n",
    "    return parameters, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1) (11, 891)\n",
      "---------------------------------\n",
      "W = [[ 0.01162427]\n",
      " [-0.01677069]\n",
      " [ 0.00570437]\n",
      " [ 0.0032844 ]\n",
      " [-0.00199015]\n",
      " [ 0.0075865 ]\n",
      " [-0.01951746]\n",
      " [ 0.00663157]\n",
      " [-0.00699314]\n",
      " [-0.00141403]\n",
      " [ 0.01889006]]\n",
      "b = -0.0114409686228\n",
      "dW = [[-0.08407305]\n",
      " [ 0.19368135]\n",
      " [-0.01022276]\n",
      " [ 0.00893097]\n",
      " [ 0.11090009]\n",
      " [-0.03997432]\n",
      " [ 0.19117187]\n",
      " [-0.04158925]\n",
      " [ 0.15945296]\n",
      " [ 0.03351812]\n",
      " [-0.11816903]]\n",
      "db = 0.109608297553\n"
     ]
    }
   ],
   "source": [
    "W, b = initialize_parameters(train_X.shape[0], train_X.shape[1])\n",
    "print(W.shape, train_X.shape)\n",
    "\n",
    "parameters, grads, costs = Optimize(W, b, train_X, train_Y, number_iterations = 100,\n",
    "                                    learning_rate = 0.001, print_cost = False)\n",
    "print(\"---------------------------------\")\n",
    "print (\"W = \" + str(parameters[\"W\"]))\n",
    "print (\"b = \" + str(parameters[\"b\"]))\n",
    "print (\"dW = \" + str(grads[\"dW\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_predict = np.zeros((1, m))\n",
    "    #print(W.shape, X.shape)\n",
    "    Z = np.dot(W.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    # print(A.shape)\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0, i] <= 0.5:\n",
    "            Y_predict[0, i] = 0\n",
    "        else:\n",
    "            Y_predict[0, i] = 1\n",
    "    assert(Y_predict.shape == (1, m))\n",
    "    return Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_X, train_Y, test_X, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    W, b = initialize_parameters(train_X.shape[0], train_X.shape[1])\n",
    "    parameters, grads, costs =  Optimize(W, b, train_X, train_Y, num_iterations, learning_rate, print_cost)\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    Y_predict_test = predict(W, b, test_X)\n",
    "    Y_predict_train = predict(W, b, train_X)\n",
    "    \n",
    "    Y_predict_test = Y_predict_test.reshape(-1, 1)\n",
    "    # print(Y_predict_test)\n",
    "    \n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_predict_train - train_Y)) * 100))\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_predict_test\": Y_predict_test,\n",
    "         \"Y_predict_train\": Y_predict_train,\n",
    "         \"W\": W,\n",
    "         \"b\": b,\n",
    "         \"learning_rate\": learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.695103\n",
      "Cost after iteration 10: 0.687076\n",
      "Cost after iteration 20: 0.679433\n",
      "Cost after iteration 30: 0.672152\n",
      "Cost after iteration 40: 0.665213\n",
      "Cost after iteration 50: 0.658597\n",
      "Cost after iteration 60: 0.652286\n",
      "Cost after iteration 70: 0.646262\n",
      "Cost after iteration 80: 0.640509\n",
      "Cost after iteration 90: 0.635013\n",
      "Cost after iteration 100: 0.629758\n",
      "Cost after iteration 110: 0.624730\n",
      "Cost after iteration 120: 0.619918\n",
      "Cost after iteration 130: 0.615310\n",
      "Cost after iteration 140: 0.610893\n",
      "Cost after iteration 150: 0.606658\n",
      "Cost after iteration 160: 0.602594\n",
      "Cost after iteration 170: 0.598693\n",
      "Cost after iteration 180: 0.594945\n",
      "Cost after iteration 190: 0.591342\n",
      "Cost after iteration 200: 0.587877\n",
      "Cost after iteration 210: 0.584543\n",
      "Cost after iteration 220: 0.581332\n",
      "Cost after iteration 230: 0.578238\n",
      "Cost after iteration 240: 0.575255\n",
      "Cost after iteration 250: 0.572378\n",
      "Cost after iteration 260: 0.569602\n",
      "Cost after iteration 270: 0.566921\n",
      "Cost after iteration 280: 0.564330\n",
      "Cost after iteration 290: 0.561826\n",
      "Cost after iteration 300: 0.559405\n",
      "Cost after iteration 310: 0.557061\n",
      "Cost after iteration 320: 0.554793\n",
      "Cost after iteration 330: 0.552595\n",
      "Cost after iteration 340: 0.550466\n",
      "Cost after iteration 350: 0.548402\n",
      "Cost after iteration 360: 0.546399\n",
      "Cost after iteration 370: 0.544456\n",
      "Cost after iteration 380: 0.542570\n",
      "Cost after iteration 390: 0.540739\n",
      "Cost after iteration 400: 0.538959\n",
      "Cost after iteration 410: 0.537229\n",
      "Cost after iteration 420: 0.535547\n",
      "Cost after iteration 430: 0.533911\n",
      "Cost after iteration 440: 0.532320\n",
      "Cost after iteration 450: 0.530770\n",
      "Cost after iteration 460: 0.529261\n",
      "Cost after iteration 470: 0.527791\n",
      "Cost after iteration 480: 0.526359\n",
      "Cost after iteration 490: 0.524963\n",
      "Cost after iteration 500: 0.523602\n",
      "Cost after iteration 510: 0.522275\n",
      "Cost after iteration 520: 0.520980\n",
      "Cost after iteration 530: 0.519716\n",
      "Cost after iteration 540: 0.518482\n",
      "Cost after iteration 550: 0.517278\n",
      "Cost after iteration 560: 0.516101\n",
      "Cost after iteration 570: 0.514951\n",
      "Cost after iteration 580: 0.513828\n",
      "Cost after iteration 590: 0.512730\n",
      "Cost after iteration 600: 0.511657\n",
      "Cost after iteration 610: 0.510607\n",
      "Cost after iteration 620: 0.509580\n",
      "Cost after iteration 630: 0.508576\n",
      "Cost after iteration 640: 0.507592\n",
      "Cost after iteration 650: 0.506630\n",
      "Cost after iteration 660: 0.505688\n",
      "Cost after iteration 670: 0.504766\n",
      "Cost after iteration 680: 0.503863\n",
      "Cost after iteration 690: 0.502978\n",
      "Cost after iteration 700: 0.502111\n",
      "Cost after iteration 710: 0.501262\n",
      "Cost after iteration 720: 0.500429\n",
      "Cost after iteration 730: 0.499613\n",
      "Cost after iteration 740: 0.498813\n",
      "Cost after iteration 750: 0.498028\n",
      "Cost after iteration 760: 0.497259\n",
      "Cost after iteration 770: 0.496504\n",
      "Cost after iteration 780: 0.495764\n",
      "Cost after iteration 790: 0.495037\n",
      "Cost after iteration 800: 0.494324\n",
      "Cost after iteration 810: 0.493624\n",
      "Cost after iteration 820: 0.492938\n",
      "Cost after iteration 830: 0.492263\n",
      "Cost after iteration 840: 0.491601\n",
      "Cost after iteration 850: 0.490951\n",
      "Cost after iteration 860: 0.490312\n",
      "Cost after iteration 870: 0.489685\n",
      "Cost after iteration 880: 0.489069\n",
      "Cost after iteration 890: 0.488464\n",
      "Cost after iteration 900: 0.487869\n",
      "Cost after iteration 910: 0.487285\n",
      "Cost after iteration 920: 0.486710\n",
      "Cost after iteration 930: 0.486145\n",
      "Cost after iteration 940: 0.485590\n",
      "Cost after iteration 950: 0.485045\n",
      "Cost after iteration 960: 0.484508\n",
      "Cost after iteration 970: 0.483981\n",
      "Cost after iteration 980: 0.483462\n",
      "Cost after iteration 990: 0.482951\n",
      "Cost after iteration 1000: 0.482449\n",
      "Cost after iteration 1010: 0.481956\n",
      "Cost after iteration 1020: 0.481470\n",
      "Cost after iteration 1030: 0.480992\n",
      "Cost after iteration 1040: 0.480521\n",
      "Cost after iteration 1050: 0.480059\n",
      "Cost after iteration 1060: 0.479603\n",
      "Cost after iteration 1070: 0.479155\n",
      "Cost after iteration 1080: 0.478714\n",
      "Cost after iteration 1090: 0.478279\n",
      "Cost after iteration 1100: 0.477852\n",
      "Cost after iteration 1110: 0.477430\n",
      "Cost after iteration 1120: 0.477016\n",
      "Cost after iteration 1130: 0.476608\n",
      "Cost after iteration 1140: 0.476206\n",
      "Cost after iteration 1150: 0.475810\n",
      "Cost after iteration 1160: 0.475420\n",
      "Cost after iteration 1170: 0.475036\n",
      "Cost after iteration 1180: 0.474657\n",
      "Cost after iteration 1190: 0.474284\n",
      "Cost after iteration 1200: 0.473917\n",
      "Cost after iteration 1210: 0.473555\n",
      "Cost after iteration 1220: 0.473199\n",
      "Cost after iteration 1230: 0.472847\n",
      "Cost after iteration 1240: 0.472501\n",
      "Cost after iteration 1250: 0.472160\n",
      "Cost after iteration 1260: 0.471824\n",
      "Cost after iteration 1270: 0.471492\n",
      "Cost after iteration 1280: 0.471165\n",
      "Cost after iteration 1290: 0.470843\n",
      "Cost after iteration 1300: 0.470526\n",
      "Cost after iteration 1310: 0.470213\n",
      "Cost after iteration 1320: 0.469904\n",
      "Cost after iteration 1330: 0.469600\n",
      "Cost after iteration 1340: 0.469299\n",
      "Cost after iteration 1350: 0.469003\n",
      "Cost after iteration 1360: 0.468712\n",
      "Cost after iteration 1370: 0.468424\n",
      "Cost after iteration 1380: 0.468140\n",
      "Cost after iteration 1390: 0.467860\n",
      "Cost after iteration 1400: 0.467583\n",
      "Cost after iteration 1410: 0.467311\n",
      "Cost after iteration 1420: 0.467042\n",
      "Cost after iteration 1430: 0.466777\n",
      "Cost after iteration 1440: 0.466515\n",
      "Cost after iteration 1450: 0.466257\n",
      "Cost after iteration 1460: 0.466002\n",
      "Cost after iteration 1470: 0.465751\n",
      "Cost after iteration 1480: 0.465503\n",
      "Cost after iteration 1490: 0.465258\n",
      "Cost after iteration 1500: 0.465016\n",
      "Cost after iteration 1510: 0.464778\n",
      "Cost after iteration 1520: 0.464543\n",
      "Cost after iteration 1530: 0.464310\n",
      "Cost after iteration 1540: 0.464081\n",
      "Cost after iteration 1550: 0.463855\n",
      "Cost after iteration 1560: 0.463631\n",
      "Cost after iteration 1570: 0.463410\n",
      "Cost after iteration 1580: 0.463193\n",
      "Cost after iteration 1590: 0.462978\n",
      "Cost after iteration 1600: 0.462765\n",
      "Cost after iteration 1610: 0.462556\n",
      "Cost after iteration 1620: 0.462348\n",
      "Cost after iteration 1630: 0.462144\n",
      "Cost after iteration 1640: 0.461942\n",
      "Cost after iteration 1650: 0.461743\n",
      "Cost after iteration 1660: 0.461546\n",
      "Cost after iteration 1670: 0.461351\n",
      "Cost after iteration 1680: 0.461159\n",
      "Cost after iteration 1690: 0.460969\n",
      "Cost after iteration 1700: 0.460782\n",
      "Cost after iteration 1710: 0.460597\n",
      "Cost after iteration 1720: 0.460414\n",
      "Cost after iteration 1730: 0.460233\n",
      "Cost after iteration 1740: 0.460054\n",
      "Cost after iteration 1750: 0.459878\n",
      "Cost after iteration 1760: 0.459704\n",
      "Cost after iteration 1770: 0.459531\n",
      "Cost after iteration 1780: 0.459361\n",
      "Cost after iteration 1790: 0.459193\n",
      "Cost after iteration 1800: 0.459027\n",
      "Cost after iteration 1810: 0.458863\n",
      "Cost after iteration 1820: 0.458700\n",
      "Cost after iteration 1830: 0.458540\n",
      "Cost after iteration 1840: 0.458381\n",
      "Cost after iteration 1850: 0.458225\n",
      "Cost after iteration 1860: 0.458070\n",
      "Cost after iteration 1870: 0.457917\n",
      "Cost after iteration 1880: 0.457765\n",
      "Cost after iteration 1890: 0.457616\n",
      "Cost after iteration 1900: 0.457468\n",
      "Cost after iteration 1910: 0.457322\n",
      "Cost after iteration 1920: 0.457177\n",
      "Cost after iteration 1930: 0.457034\n",
      "Cost after iteration 1940: 0.456893\n",
      "Cost after iteration 1950: 0.456753\n",
      "Cost after iteration 1960: 0.456615\n",
      "Cost after iteration 1970: 0.456479\n",
      "Cost after iteration 1980: 0.456344\n",
      "Cost after iteration 1990: 0.456210\n",
      "Cost after iteration 2000: 0.456078\n",
      "Cost after iteration 2010: 0.455947\n",
      "Cost after iteration 2020: 0.455818\n",
      "Cost after iteration 2030: 0.455690\n",
      "Cost after iteration 2040: 0.455564\n",
      "Cost after iteration 2050: 0.455439\n",
      "Cost after iteration 2060: 0.455315\n",
      "Cost after iteration 2070: 0.455193\n",
      "Cost after iteration 2080: 0.455072\n",
      "Cost after iteration 2090: 0.454952\n",
      "Cost after iteration 2100: 0.454834\n",
      "Cost after iteration 2110: 0.454717\n",
      "Cost after iteration 2120: 0.454601\n",
      "Cost after iteration 2130: 0.454486\n",
      "Cost after iteration 2140: 0.454373\n",
      "Cost after iteration 2150: 0.454261\n",
      "Cost after iteration 2160: 0.454150\n",
      "Cost after iteration 2170: 0.454040\n",
      "Cost after iteration 2180: 0.453931\n",
      "Cost after iteration 2190: 0.453824\n",
      "Cost after iteration 2200: 0.453717\n",
      "Cost after iteration 2210: 0.453612\n",
      "Cost after iteration 2220: 0.453508\n",
      "Cost after iteration 2230: 0.453404\n",
      "Cost after iteration 2240: 0.453302\n",
      "Cost after iteration 2250: 0.453201\n",
      "Cost after iteration 2260: 0.453101\n",
      "Cost after iteration 2270: 0.453002\n",
      "Cost after iteration 2280: 0.452904\n",
      "Cost after iteration 2290: 0.452807\n",
      "Cost after iteration 2300: 0.452711\n",
      "Cost after iteration 2310: 0.452616\n",
      "Cost after iteration 2320: 0.452522\n",
      "Cost after iteration 2330: 0.452429\n",
      "Cost after iteration 2340: 0.452337\n",
      "Cost after iteration 2350: 0.452246\n",
      "Cost after iteration 2360: 0.452155\n",
      "Cost after iteration 2370: 0.452066\n",
      "Cost after iteration 2380: 0.451977\n",
      "Cost after iteration 2390: 0.451890\n",
      "Cost after iteration 2400: 0.451803\n",
      "Cost after iteration 2410: 0.451717\n",
      "Cost after iteration 2420: 0.451632\n",
      "Cost after iteration 2430: 0.451547\n",
      "Cost after iteration 2440: 0.451464\n",
      "Cost after iteration 2450: 0.451381\n",
      "Cost after iteration 2460: 0.451299\n",
      "Cost after iteration 2470: 0.451218\n",
      "Cost after iteration 2480: 0.451138\n",
      "Cost after iteration 2490: 0.451059\n",
      "Cost after iteration 2500: 0.450980\n",
      "Cost after iteration 2510: 0.450902\n",
      "Cost after iteration 2520: 0.450825\n",
      "Cost after iteration 2530: 0.450748\n",
      "Cost after iteration 2540: 0.450672\n",
      "Cost after iteration 2550: 0.450597\n",
      "Cost after iteration 2560: 0.450523\n",
      "Cost after iteration 2570: 0.450449\n",
      "Cost after iteration 2580: 0.450376\n",
      "Cost after iteration 2590: 0.450304\n",
      "Cost after iteration 2600: 0.450232\n",
      "Cost after iteration 2610: 0.450161\n",
      "Cost after iteration 2620: 0.450091\n",
      "Cost after iteration 2630: 0.450021\n",
      "Cost after iteration 2640: 0.449952\n",
      "Cost after iteration 2650: 0.449884\n",
      "Cost after iteration 2660: 0.449816\n",
      "Cost after iteration 2670: 0.449749\n",
      "Cost after iteration 2680: 0.449683\n",
      "Cost after iteration 2690: 0.449617\n",
      "Cost after iteration 2700: 0.449551\n",
      "Cost after iteration 2710: 0.449487\n",
      "Cost after iteration 2720: 0.449423\n",
      "Cost after iteration 2730: 0.449359\n",
      "Cost after iteration 2740: 0.449296\n",
      "Cost after iteration 2750: 0.449234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 2760: 0.449172\n",
      "Cost after iteration 2770: 0.449110\n",
      "Cost after iteration 2780: 0.449050\n",
      "Cost after iteration 2790: 0.448989\n",
      "Cost after iteration 2800: 0.448930\n",
      "Cost after iteration 2810: 0.448871\n",
      "Cost after iteration 2820: 0.448812\n",
      "Cost after iteration 2830: 0.448754\n",
      "Cost after iteration 2840: 0.448696\n",
      "Cost after iteration 2850: 0.448639\n",
      "Cost after iteration 2860: 0.448582\n",
      "Cost after iteration 2870: 0.448526\n",
      "Cost after iteration 2880: 0.448471\n",
      "Cost after iteration 2890: 0.448416\n",
      "Cost after iteration 2900: 0.448361\n",
      "Cost after iteration 2910: 0.448307\n",
      "Cost after iteration 2920: 0.448253\n",
      "Cost after iteration 2930: 0.448200\n",
      "Cost after iteration 2940: 0.448147\n",
      "Cost after iteration 2950: 0.448094\n",
      "Cost after iteration 2960: 0.448042\n",
      "Cost after iteration 2970: 0.447991\n",
      "Cost after iteration 2980: 0.447940\n",
      "Cost after iteration 2990: 0.447889\n",
      "Cost after iteration 3000: 0.447839\n",
      "Cost after iteration 3010: 0.447789\n",
      "Cost after iteration 3020: 0.447740\n",
      "Cost after iteration 3030: 0.447691\n",
      "Cost after iteration 3040: 0.447642\n",
      "Cost after iteration 3050: 0.447594\n",
      "Cost after iteration 3060: 0.447547\n",
      "Cost after iteration 3070: 0.447499\n",
      "Cost after iteration 3080: 0.447452\n",
      "Cost after iteration 3090: 0.447406\n",
      "Cost after iteration 3100: 0.447359\n",
      "Cost after iteration 3110: 0.447314\n",
      "Cost after iteration 3120: 0.447268\n",
      "Cost after iteration 3130: 0.447223\n",
      "Cost after iteration 3140: 0.447178\n",
      "Cost after iteration 3150: 0.447134\n",
      "Cost after iteration 3160: 0.447090\n",
      "Cost after iteration 3170: 0.447046\n",
      "Cost after iteration 3180: 0.447003\n",
      "Cost after iteration 3190: 0.446960\n",
      "Cost after iteration 3200: 0.446918\n",
      "Cost after iteration 3210: 0.446875\n",
      "Cost after iteration 3220: 0.446833\n",
      "Cost after iteration 3230: 0.446792\n",
      "Cost after iteration 3240: 0.446751\n",
      "Cost after iteration 3250: 0.446710\n",
      "Cost after iteration 3260: 0.446669\n",
      "Cost after iteration 3270: 0.446629\n",
      "Cost after iteration 3280: 0.446589\n",
      "Cost after iteration 3290: 0.446549\n",
      "Cost after iteration 3300: 0.446510\n",
      "Cost after iteration 3310: 0.446471\n",
      "Cost after iteration 3320: 0.446432\n",
      "Cost after iteration 3330: 0.446394\n",
      "Cost after iteration 3340: 0.446356\n",
      "Cost after iteration 3350: 0.446318\n",
      "Cost after iteration 3360: 0.446280\n",
      "Cost after iteration 3370: 0.446243\n",
      "Cost after iteration 3380: 0.446206\n",
      "Cost after iteration 3390: 0.446169\n",
      "Cost after iteration 3400: 0.446133\n",
      "Cost after iteration 3410: 0.446097\n",
      "Cost after iteration 3420: 0.446061\n",
      "Cost after iteration 3430: 0.446025\n",
      "Cost after iteration 3440: 0.445990\n",
      "Cost after iteration 3450: 0.445955\n",
      "Cost after iteration 3460: 0.445920\n",
      "Cost after iteration 3470: 0.445886\n",
      "Cost after iteration 3480: 0.445851\n",
      "Cost after iteration 3490: 0.445817\n",
      "Cost after iteration 3500: 0.445784\n",
      "Cost after iteration 3510: 0.445750\n",
      "Cost after iteration 3520: 0.445717\n",
      "Cost after iteration 3530: 0.445684\n",
      "Cost after iteration 3540: 0.445651\n",
      "Cost after iteration 3550: 0.445619\n",
      "Cost after iteration 3560: 0.445586\n",
      "Cost after iteration 3570: 0.445554\n",
      "Cost after iteration 3580: 0.445522\n",
      "Cost after iteration 3590: 0.445491\n",
      "Cost after iteration 3600: 0.445459\n",
      "Cost after iteration 3610: 0.445428\n",
      "Cost after iteration 3620: 0.445397\n",
      "Cost after iteration 3630: 0.445367\n",
      "Cost after iteration 3640: 0.445336\n",
      "Cost after iteration 3650: 0.445306\n",
      "Cost after iteration 3660: 0.445276\n",
      "Cost after iteration 3670: 0.445246\n",
      "Cost after iteration 3680: 0.445217\n",
      "Cost after iteration 3690: 0.445187\n",
      "Cost after iteration 3700: 0.445158\n",
      "Cost after iteration 3710: 0.445129\n",
      "Cost after iteration 3720: 0.445100\n",
      "Cost after iteration 3730: 0.445072\n",
      "Cost after iteration 3740: 0.445044\n",
      "Cost after iteration 3750: 0.445015\n",
      "Cost after iteration 3760: 0.444987\n",
      "Cost after iteration 3770: 0.444960\n",
      "Cost after iteration 3780: 0.444932\n",
      "Cost after iteration 3790: 0.444905\n",
      "Cost after iteration 3800: 0.444878\n",
      "Cost after iteration 3810: 0.444851\n",
      "Cost after iteration 3820: 0.444824\n",
      "Cost after iteration 3830: 0.444797\n",
      "Cost after iteration 3840: 0.444771\n",
      "Cost after iteration 3850: 0.444745\n",
      "Cost after iteration 3860: 0.444718\n",
      "Cost after iteration 3870: 0.444693\n",
      "Cost after iteration 3880: 0.444667\n",
      "Cost after iteration 3890: 0.444641\n",
      "Cost after iteration 3900: 0.444616\n",
      "Cost after iteration 3910: 0.444591\n",
      "Cost after iteration 3920: 0.444566\n",
      "Cost after iteration 3930: 0.444541\n",
      "Cost after iteration 3940: 0.444516\n",
      "Cost after iteration 3950: 0.444492\n",
      "Cost after iteration 3960: 0.444467\n",
      "Cost after iteration 3970: 0.444443\n",
      "Cost after iteration 3980: 0.444419\n",
      "Cost after iteration 3990: 0.444395\n",
      "Cost after iteration 4000: 0.444372\n",
      "Cost after iteration 4010: 0.444348\n",
      "Cost after iteration 4020: 0.444325\n",
      "Cost after iteration 4030: 0.444302\n",
      "Cost after iteration 4040: 0.444278\n",
      "Cost after iteration 4050: 0.444256\n",
      "Cost after iteration 4060: 0.444233\n",
      "Cost after iteration 4070: 0.444210\n",
      "Cost after iteration 4080: 0.444188\n",
      "Cost after iteration 4090: 0.444165\n",
      "Cost after iteration 4100: 0.444143\n",
      "Cost after iteration 4110: 0.444121\n",
      "Cost after iteration 4120: 0.444099\n",
      "Cost after iteration 4130: 0.444078\n",
      "Cost after iteration 4140: 0.444056\n",
      "Cost after iteration 4150: 0.444035\n",
      "Cost after iteration 4160: 0.444013\n",
      "Cost after iteration 4170: 0.443992\n",
      "Cost after iteration 4180: 0.443971\n",
      "Cost after iteration 4190: 0.443950\n",
      "Cost after iteration 4200: 0.443929\n",
      "Cost after iteration 4210: 0.443909\n",
      "Cost after iteration 4220: 0.443888\n",
      "Cost after iteration 4230: 0.443868\n",
      "Cost after iteration 4240: 0.443848\n",
      "Cost after iteration 4250: 0.443827\n",
      "Cost after iteration 4260: 0.443807\n",
      "Cost after iteration 4270: 0.443788\n",
      "Cost after iteration 4280: 0.443768\n",
      "Cost after iteration 4290: 0.443748\n",
      "Cost after iteration 4300: 0.443729\n",
      "Cost after iteration 4310: 0.443709\n",
      "Cost after iteration 4320: 0.443690\n",
      "Cost after iteration 4330: 0.443671\n",
      "Cost after iteration 4340: 0.443652\n",
      "Cost after iteration 4350: 0.443633\n",
      "Cost after iteration 4360: 0.443614\n",
      "Cost after iteration 4370: 0.443596\n",
      "Cost after iteration 4380: 0.443577\n",
      "Cost after iteration 4390: 0.443559\n",
      "Cost after iteration 4400: 0.443540\n",
      "Cost after iteration 4410: 0.443522\n",
      "Cost after iteration 4420: 0.443504\n",
      "Cost after iteration 4430: 0.443486\n",
      "Cost after iteration 4440: 0.443468\n",
      "Cost after iteration 4450: 0.443451\n",
      "Cost after iteration 4460: 0.443433\n",
      "Cost after iteration 4470: 0.443415\n",
      "Cost after iteration 4480: 0.443398\n",
      "Cost after iteration 4490: 0.443381\n",
      "Cost after iteration 4500: 0.443363\n",
      "Cost after iteration 4510: 0.443346\n",
      "Cost after iteration 4520: 0.443329\n",
      "Cost after iteration 4530: 0.443312\n",
      "Cost after iteration 4540: 0.443296\n",
      "Cost after iteration 4550: 0.443279\n",
      "Cost after iteration 4560: 0.443262\n",
      "Cost after iteration 4570: 0.443246\n",
      "Cost after iteration 4580: 0.443229\n",
      "Cost after iteration 4590: 0.443213\n",
      "Cost after iteration 4600: 0.443197\n",
      "Cost after iteration 4610: 0.443181\n",
      "Cost after iteration 4620: 0.443165\n",
      "Cost after iteration 4630: 0.443149\n",
      "Cost after iteration 4640: 0.443133\n",
      "Cost after iteration 4650: 0.443117\n",
      "Cost after iteration 4660: 0.443101\n",
      "Cost after iteration 4670: 0.443086\n",
      "Cost after iteration 4680: 0.443070\n",
      "Cost after iteration 4690: 0.443055\n",
      "Cost after iteration 4700: 0.443040\n",
      "Cost after iteration 4710: 0.443025\n",
      "Cost after iteration 4720: 0.443010\n",
      "Cost after iteration 4730: 0.442994\n",
      "Cost after iteration 4740: 0.442980\n",
      "Cost after iteration 4750: 0.442965\n",
      "Cost after iteration 4760: 0.442950\n",
      "Cost after iteration 4770: 0.442935\n",
      "Cost after iteration 4780: 0.442921\n",
      "Cost after iteration 4790: 0.442906\n",
      "Cost after iteration 4800: 0.442892\n",
      "Cost after iteration 4810: 0.442877\n",
      "Cost after iteration 4820: 0.442863\n",
      "Cost after iteration 4830: 0.442849\n",
      "Cost after iteration 4840: 0.442835\n",
      "Cost after iteration 4850: 0.442821\n",
      "Cost after iteration 4860: 0.442807\n",
      "Cost after iteration 4870: 0.442793\n",
      "Cost after iteration 4880: 0.442779\n",
      "Cost after iteration 4890: 0.442766\n",
      "Cost after iteration 4900: 0.442752\n",
      "Cost after iteration 4910: 0.442738\n",
      "Cost after iteration 4920: 0.442725\n",
      "Cost after iteration 4930: 0.442712\n",
      "Cost after iteration 4940: 0.442698\n",
      "Cost after iteration 4950: 0.442685\n",
      "Cost after iteration 4960: 0.442672\n",
      "Cost after iteration 4970: 0.442659\n",
      "Cost after iteration 4980: 0.442646\n",
      "Cost after iteration 4990: 0.442633\n",
      "Cost after iteration 5000: 0.442620\n",
      "Cost after iteration 5010: 0.442607\n",
      "Cost after iteration 5020: 0.442594\n",
      "Cost after iteration 5030: 0.442582\n",
      "Cost after iteration 5040: 0.442569\n",
      "Cost after iteration 5050: 0.442557\n",
      "Cost after iteration 5060: 0.442544\n",
      "Cost after iteration 5070: 0.442532\n",
      "Cost after iteration 5080: 0.442519\n",
      "Cost after iteration 5090: 0.442507\n",
      "Cost after iteration 5100: 0.442495\n",
      "Cost after iteration 5110: 0.442483\n",
      "Cost after iteration 5120: 0.442471\n",
      "Cost after iteration 5130: 0.442459\n",
      "Cost after iteration 5140: 0.442447\n",
      "Cost after iteration 5150: 0.442435\n",
      "Cost after iteration 5160: 0.442423\n",
      "Cost after iteration 5170: 0.442411\n",
      "Cost after iteration 5180: 0.442400\n",
      "Cost after iteration 5190: 0.442388\n",
      "Cost after iteration 5200: 0.442377\n",
      "Cost after iteration 5210: 0.442365\n",
      "Cost after iteration 5220: 0.442354\n",
      "Cost after iteration 5230: 0.442342\n",
      "Cost after iteration 5240: 0.442331\n",
      "Cost after iteration 5250: 0.442320\n",
      "Cost after iteration 5260: 0.442309\n",
      "Cost after iteration 5270: 0.442298\n",
      "Cost after iteration 5280: 0.442286\n",
      "Cost after iteration 5290: 0.442275\n",
      "Cost after iteration 5300: 0.442265\n",
      "Cost after iteration 5310: 0.442254\n",
      "Cost after iteration 5320: 0.442243\n",
      "Cost after iteration 5330: 0.442232\n",
      "Cost after iteration 5340: 0.442221\n",
      "Cost after iteration 5350: 0.442211\n",
      "Cost after iteration 5360: 0.442200\n",
      "Cost after iteration 5370: 0.442189\n",
      "Cost after iteration 5380: 0.442179\n",
      "Cost after iteration 5390: 0.442169\n",
      "Cost after iteration 5400: 0.442158\n",
      "Cost after iteration 5410: 0.442148\n",
      "Cost after iteration 5420: 0.442138\n",
      "Cost after iteration 5430: 0.442127\n",
      "Cost after iteration 5440: 0.442117\n",
      "Cost after iteration 5450: 0.442107\n",
      "Cost after iteration 5460: 0.442097\n",
      "Cost after iteration 5470: 0.442087\n",
      "Cost after iteration 5480: 0.442077\n",
      "Cost after iteration 5490: 0.442067\n",
      "Cost after iteration 5500: 0.442057\n",
      "Cost after iteration 5510: 0.442047\n",
      "Cost after iteration 5520: 0.442038\n",
      "Cost after iteration 5530: 0.442028\n",
      "Cost after iteration 5540: 0.442018\n",
      "Cost after iteration 5550: 0.442009\n",
      "Cost after iteration 5560: 0.441999\n",
      "Cost after iteration 5570: 0.441989\n",
      "Cost after iteration 5580: 0.441980\n",
      "Cost after iteration 5590: 0.441971\n",
      "Cost after iteration 5600: 0.441961\n",
      "Cost after iteration 5610: 0.441952\n",
      "Cost after iteration 5620: 0.441943\n",
      "Cost after iteration 5630: 0.441933\n",
      "Cost after iteration 5640: 0.441924\n",
      "Cost after iteration 5650: 0.441915\n",
      "Cost after iteration 5660: 0.441906\n",
      "Cost after iteration 5670: 0.441897\n",
      "Cost after iteration 5680: 0.441888\n",
      "Cost after iteration 5690: 0.441879\n",
      "Cost after iteration 5700: 0.441870\n",
      "Cost after iteration 5710: 0.441861\n",
      "Cost after iteration 5720: 0.441852\n",
      "Cost after iteration 5730: 0.441843\n",
      "Cost after iteration 5740: 0.441835\n",
      "Cost after iteration 5750: 0.441826\n",
      "Cost after iteration 5760: 0.441817\n",
      "Cost after iteration 5770: 0.441809\n",
      "Cost after iteration 5780: 0.441800\n",
      "Cost after iteration 5790: 0.441792\n",
      "Cost after iteration 5800: 0.441783\n",
      "Cost after iteration 5810: 0.441775\n",
      "Cost after iteration 5820: 0.441766\n",
      "Cost after iteration 5830: 0.441758\n",
      "Cost after iteration 5840: 0.441750\n",
      "Cost after iteration 5850: 0.441741\n",
      "Cost after iteration 5860: 0.441733\n",
      "Cost after iteration 5870: 0.441725\n",
      "Cost after iteration 5880: 0.441717\n",
      "Cost after iteration 5890: 0.441708\n",
      "Cost after iteration 5900: 0.441700\n",
      "Cost after iteration 5910: 0.441692\n",
      "Cost after iteration 5920: 0.441684\n",
      "Cost after iteration 5930: 0.441676\n",
      "Cost after iteration 5940: 0.441668\n",
      "Cost after iteration 5950: 0.441661\n",
      "Cost after iteration 5960: 0.441653\n",
      "Cost after iteration 5970: 0.441645\n",
      "Cost after iteration 5980: 0.441637\n",
      "Cost after iteration 5990: 0.441629\n",
      "Cost after iteration 6000: 0.441622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 6010: 0.441614\n",
      "Cost after iteration 6020: 0.441606\n",
      "Cost after iteration 6030: 0.441599\n",
      "Cost after iteration 6040: 0.441591\n",
      "Cost after iteration 6050: 0.441584\n",
      "Cost after iteration 6060: 0.441576\n",
      "Cost after iteration 6070: 0.441569\n",
      "Cost after iteration 6080: 0.441561\n",
      "Cost after iteration 6090: 0.441554\n",
      "Cost after iteration 6100: 0.441546\n",
      "Cost after iteration 6110: 0.441539\n",
      "Cost after iteration 6120: 0.441532\n",
      "Cost after iteration 6130: 0.441525\n",
      "Cost after iteration 6140: 0.441517\n",
      "Cost after iteration 6150: 0.441510\n",
      "Cost after iteration 6160: 0.441503\n",
      "Cost after iteration 6170: 0.441496\n",
      "Cost after iteration 6180: 0.441489\n",
      "Cost after iteration 6190: 0.441482\n",
      "Cost after iteration 6200: 0.441475\n",
      "Cost after iteration 6210: 0.441468\n",
      "Cost after iteration 6220: 0.441461\n",
      "Cost after iteration 6230: 0.441454\n",
      "Cost after iteration 6240: 0.441447\n",
      "Cost after iteration 6250: 0.441440\n",
      "Cost after iteration 6260: 0.441433\n",
      "Cost after iteration 6270: 0.441426\n",
      "Cost after iteration 6280: 0.441420\n",
      "Cost after iteration 6290: 0.441413\n",
      "Cost after iteration 6300: 0.441406\n",
      "Cost after iteration 6310: 0.441399\n",
      "Cost after iteration 6320: 0.441393\n",
      "Cost after iteration 6330: 0.441386\n",
      "Cost after iteration 6340: 0.441380\n",
      "Cost after iteration 6350: 0.441373\n",
      "Cost after iteration 6360: 0.441366\n",
      "Cost after iteration 6370: 0.441360\n",
      "Cost after iteration 6380: 0.441353\n",
      "Cost after iteration 6390: 0.441347\n",
      "Cost after iteration 6400: 0.441341\n",
      "Cost after iteration 6410: 0.441334\n",
      "Cost after iteration 6420: 0.441328\n",
      "Cost after iteration 6430: 0.441322\n",
      "Cost after iteration 6440: 0.441315\n",
      "Cost after iteration 6450: 0.441309\n",
      "Cost after iteration 6460: 0.441303\n",
      "Cost after iteration 6470: 0.441296\n",
      "Cost after iteration 6480: 0.441290\n",
      "Cost after iteration 6490: 0.441284\n",
      "Cost after iteration 6500: 0.441278\n",
      "Cost after iteration 6510: 0.441272\n",
      "Cost after iteration 6520: 0.441266\n",
      "Cost after iteration 6530: 0.441260\n",
      "Cost after iteration 6540: 0.441254\n",
      "Cost after iteration 6550: 0.441248\n",
      "Cost after iteration 6560: 0.441242\n",
      "Cost after iteration 6570: 0.441236\n",
      "Cost after iteration 6580: 0.441230\n",
      "Cost after iteration 6590: 0.441224\n",
      "Cost after iteration 6600: 0.441218\n",
      "Cost after iteration 6610: 0.441212\n",
      "Cost after iteration 6620: 0.441206\n",
      "Cost after iteration 6630: 0.441201\n",
      "Cost after iteration 6640: 0.441195\n",
      "Cost after iteration 6650: 0.441189\n",
      "Cost after iteration 6660: 0.441183\n",
      "Cost after iteration 6670: 0.441178\n",
      "Cost after iteration 6680: 0.441172\n",
      "Cost after iteration 6690: 0.441166\n",
      "Cost after iteration 6700: 0.441161\n",
      "Cost after iteration 6710: 0.441155\n",
      "Cost after iteration 6720: 0.441149\n",
      "Cost after iteration 6730: 0.441144\n",
      "Cost after iteration 6740: 0.441138\n",
      "Cost after iteration 6750: 0.441133\n",
      "Cost after iteration 6760: 0.441127\n",
      "Cost after iteration 6770: 0.441122\n",
      "Cost after iteration 6780: 0.441116\n",
      "Cost after iteration 6790: 0.441111\n",
      "Cost after iteration 6800: 0.441106\n",
      "Cost after iteration 6810: 0.441100\n",
      "Cost after iteration 6820: 0.441095\n",
      "Cost after iteration 6830: 0.441090\n",
      "Cost after iteration 6840: 0.441084\n",
      "Cost after iteration 6850: 0.441079\n",
      "Cost after iteration 6860: 0.441074\n",
      "Cost after iteration 6870: 0.441068\n",
      "Cost after iteration 6880: 0.441063\n",
      "Cost after iteration 6890: 0.441058\n",
      "Cost after iteration 6900: 0.441053\n",
      "Cost after iteration 6910: 0.441048\n",
      "Cost after iteration 6920: 0.441043\n",
      "Cost after iteration 6930: 0.441038\n",
      "Cost after iteration 6940: 0.441032\n",
      "Cost after iteration 6950: 0.441027\n",
      "Cost after iteration 6960: 0.441022\n",
      "Cost after iteration 6970: 0.441017\n",
      "Cost after iteration 6980: 0.441012\n",
      "Cost after iteration 6990: 0.441007\n",
      "Cost after iteration 7000: 0.441002\n",
      "Cost after iteration 7010: 0.440997\n",
      "Cost after iteration 7020: 0.440992\n",
      "Cost after iteration 7030: 0.440988\n",
      "Cost after iteration 7040: 0.440983\n",
      "Cost after iteration 7050: 0.440978\n",
      "Cost after iteration 7060: 0.440973\n",
      "Cost after iteration 7070: 0.440968\n",
      "Cost after iteration 7080: 0.440963\n",
      "Cost after iteration 7090: 0.440959\n",
      "Cost after iteration 7100: 0.440954\n",
      "Cost after iteration 7110: 0.440949\n",
      "Cost after iteration 7120: 0.440944\n",
      "Cost after iteration 7130: 0.440940\n",
      "Cost after iteration 7140: 0.440935\n",
      "Cost after iteration 7150: 0.440930\n",
      "Cost after iteration 7160: 0.440926\n",
      "Cost after iteration 7170: 0.440921\n",
      "Cost after iteration 7180: 0.440916\n",
      "Cost after iteration 7190: 0.440912\n",
      "Cost after iteration 7200: 0.440907\n",
      "Cost after iteration 7210: 0.440903\n",
      "Cost after iteration 7220: 0.440898\n",
      "Cost after iteration 7230: 0.440894\n",
      "Cost after iteration 7240: 0.440889\n",
      "Cost after iteration 7250: 0.440885\n",
      "Cost after iteration 7260: 0.440880\n",
      "Cost after iteration 7270: 0.440876\n",
      "Cost after iteration 7280: 0.440871\n",
      "Cost after iteration 7290: 0.440867\n",
      "Cost after iteration 7300: 0.440862\n",
      "Cost after iteration 7310: 0.440858\n",
      "Cost after iteration 7320: 0.440854\n",
      "Cost after iteration 7330: 0.440849\n",
      "Cost after iteration 7340: 0.440845\n",
      "Cost after iteration 7350: 0.440841\n",
      "Cost after iteration 7360: 0.440836\n",
      "Cost after iteration 7370: 0.440832\n",
      "Cost after iteration 7380: 0.440828\n",
      "Cost after iteration 7390: 0.440824\n",
      "Cost after iteration 7400: 0.440819\n",
      "Cost after iteration 7410: 0.440815\n",
      "Cost after iteration 7420: 0.440811\n",
      "Cost after iteration 7430: 0.440807\n",
      "Cost after iteration 7440: 0.440803\n",
      "Cost after iteration 7450: 0.440798\n",
      "Cost after iteration 7460: 0.440794\n",
      "Cost after iteration 7470: 0.440790\n",
      "Cost after iteration 7480: 0.440786\n",
      "Cost after iteration 7490: 0.440782\n",
      "Cost after iteration 7500: 0.440778\n",
      "Cost after iteration 7510: 0.440774\n",
      "Cost after iteration 7520: 0.440770\n",
      "Cost after iteration 7530: 0.440766\n",
      "Cost after iteration 7540: 0.440762\n",
      "Cost after iteration 7550: 0.440758\n",
      "Cost after iteration 7560: 0.440754\n",
      "Cost after iteration 7570: 0.440750\n",
      "Cost after iteration 7580: 0.440746\n",
      "Cost after iteration 7590: 0.440742\n",
      "Cost after iteration 7600: 0.440738\n",
      "Cost after iteration 7610: 0.440734\n",
      "Cost after iteration 7620: 0.440730\n",
      "Cost after iteration 7630: 0.440727\n",
      "Cost after iteration 7640: 0.440723\n",
      "Cost after iteration 7650: 0.440719\n",
      "Cost after iteration 7660: 0.440715\n",
      "Cost after iteration 7670: 0.440711\n",
      "Cost after iteration 7680: 0.440707\n",
      "Cost after iteration 7690: 0.440704\n",
      "Cost after iteration 7700: 0.440700\n",
      "Cost after iteration 7710: 0.440696\n",
      "Cost after iteration 7720: 0.440692\n",
      "Cost after iteration 7730: 0.440689\n",
      "Cost after iteration 7740: 0.440685\n",
      "Cost after iteration 7750: 0.440681\n",
      "Cost after iteration 7760: 0.440677\n",
      "Cost after iteration 7770: 0.440674\n",
      "Cost after iteration 7780: 0.440670\n",
      "Cost after iteration 7790: 0.440667\n",
      "Cost after iteration 7800: 0.440663\n",
      "Cost after iteration 7810: 0.440659\n",
      "Cost after iteration 7820: 0.440656\n",
      "Cost after iteration 7830: 0.440652\n",
      "Cost after iteration 7840: 0.440648\n",
      "Cost after iteration 7850: 0.440645\n",
      "Cost after iteration 7860: 0.440641\n",
      "Cost after iteration 7870: 0.440638\n",
      "Cost after iteration 7880: 0.440634\n",
      "Cost after iteration 7890: 0.440631\n",
      "Cost after iteration 7900: 0.440627\n",
      "Cost after iteration 7910: 0.440624\n",
      "Cost after iteration 7920: 0.440620\n",
      "Cost after iteration 7930: 0.440617\n",
      "Cost after iteration 7940: 0.440613\n",
      "Cost after iteration 7950: 0.440610\n",
      "Cost after iteration 7960: 0.440607\n",
      "Cost after iteration 7970: 0.440603\n",
      "Cost after iteration 7980: 0.440600\n",
      "Cost after iteration 7990: 0.440596\n",
      "Cost after iteration 8000: 0.440593\n",
      "Cost after iteration 8010: 0.440590\n",
      "Cost after iteration 8020: 0.440586\n",
      "Cost after iteration 8030: 0.440583\n",
      "Cost after iteration 8040: 0.440580\n",
      "Cost after iteration 8050: 0.440576\n",
      "Cost after iteration 8060: 0.440573\n",
      "Cost after iteration 8070: 0.440570\n",
      "Cost after iteration 8080: 0.440566\n",
      "Cost after iteration 8090: 0.440563\n",
      "Cost after iteration 8100: 0.440560\n",
      "Cost after iteration 8110: 0.440557\n",
      "Cost after iteration 8120: 0.440553\n",
      "Cost after iteration 8130: 0.440550\n",
      "Cost after iteration 8140: 0.440547\n",
      "Cost after iteration 8150: 0.440544\n",
      "Cost after iteration 8160: 0.440541\n",
      "Cost after iteration 8170: 0.440537\n",
      "Cost after iteration 8180: 0.440534\n",
      "Cost after iteration 8190: 0.440531\n",
      "Cost after iteration 8200: 0.440528\n",
      "Cost after iteration 8210: 0.440525\n",
      "Cost after iteration 8220: 0.440522\n",
      "Cost after iteration 8230: 0.440519\n",
      "Cost after iteration 8240: 0.440515\n",
      "Cost after iteration 8250: 0.440512\n",
      "Cost after iteration 8260: 0.440509\n",
      "Cost after iteration 8270: 0.440506\n",
      "Cost after iteration 8280: 0.440503\n",
      "Cost after iteration 8290: 0.440500\n",
      "Cost after iteration 8300: 0.440497\n",
      "Cost after iteration 8310: 0.440494\n",
      "Cost after iteration 8320: 0.440491\n",
      "Cost after iteration 8330: 0.440488\n",
      "Cost after iteration 8340: 0.440485\n",
      "Cost after iteration 8350: 0.440482\n",
      "Cost after iteration 8360: 0.440479\n",
      "Cost after iteration 8370: 0.440476\n",
      "Cost after iteration 8380: 0.440473\n",
      "Cost after iteration 8390: 0.440470\n",
      "Cost after iteration 8400: 0.440467\n",
      "Cost after iteration 8410: 0.440464\n",
      "Cost after iteration 8420: 0.440462\n",
      "Cost after iteration 8430: 0.440459\n",
      "Cost after iteration 8440: 0.440456\n",
      "Cost after iteration 8450: 0.440453\n",
      "Cost after iteration 8460: 0.440450\n",
      "Cost after iteration 8470: 0.440447\n",
      "Cost after iteration 8480: 0.440444\n",
      "Cost after iteration 8490: 0.440441\n",
      "Cost after iteration 8500: 0.440439\n",
      "Cost after iteration 8510: 0.440436\n",
      "Cost after iteration 8520: 0.440433\n",
      "Cost after iteration 8530: 0.440430\n",
      "Cost after iteration 8540: 0.440427\n",
      "Cost after iteration 8550: 0.440425\n",
      "Cost after iteration 8560: 0.440422\n",
      "Cost after iteration 8570: 0.440419\n",
      "Cost after iteration 8580: 0.440416\n",
      "Cost after iteration 8590: 0.440413\n",
      "Cost after iteration 8600: 0.440411\n",
      "Cost after iteration 8610: 0.440408\n",
      "Cost after iteration 8620: 0.440405\n",
      "Cost after iteration 8630: 0.440403\n",
      "Cost after iteration 8640: 0.440400\n",
      "Cost after iteration 8650: 0.440397\n",
      "Cost after iteration 8660: 0.440395\n",
      "Cost after iteration 8670: 0.440392\n",
      "Cost after iteration 8680: 0.440389\n",
      "Cost after iteration 8690: 0.440387\n",
      "Cost after iteration 8700: 0.440384\n",
      "Cost after iteration 8710: 0.440381\n",
      "Cost after iteration 8720: 0.440379\n",
      "Cost after iteration 8730: 0.440376\n",
      "Cost after iteration 8740: 0.440373\n",
      "Cost after iteration 8750: 0.440371\n",
      "Cost after iteration 8760: 0.440368\n",
      "Cost after iteration 8770: 0.440366\n",
      "Cost after iteration 8780: 0.440363\n",
      "Cost after iteration 8790: 0.440360\n",
      "Cost after iteration 8800: 0.440358\n",
      "Cost after iteration 8810: 0.440355\n",
      "Cost after iteration 8820: 0.440353\n",
      "Cost after iteration 8830: 0.440350\n",
      "Cost after iteration 8840: 0.440348\n",
      "Cost after iteration 8850: 0.440345\n",
      "Cost after iteration 8860: 0.440343\n",
      "Cost after iteration 8870: 0.440340\n",
      "Cost after iteration 8880: 0.440338\n",
      "Cost after iteration 8890: 0.440335\n",
      "Cost after iteration 8900: 0.440333\n",
      "Cost after iteration 8910: 0.440330\n",
      "Cost after iteration 8920: 0.440328\n",
      "Cost after iteration 8930: 0.440325\n",
      "Cost after iteration 8940: 0.440323\n",
      "Cost after iteration 8950: 0.440320\n",
      "Cost after iteration 8960: 0.440318\n",
      "Cost after iteration 8970: 0.440316\n",
      "Cost after iteration 8980: 0.440313\n",
      "Cost after iteration 8990: 0.440311\n",
      "Cost after iteration 9000: 0.440308\n",
      "Cost after iteration 9010: 0.440306\n",
      "Cost after iteration 9020: 0.440304\n",
      "Cost after iteration 9030: 0.440301\n",
      "Cost after iteration 9040: 0.440299\n",
      "Cost after iteration 9050: 0.440296\n",
      "Cost after iteration 9060: 0.440294\n",
      "Cost after iteration 9070: 0.440292\n",
      "Cost after iteration 9080: 0.440289\n",
      "Cost after iteration 9090: 0.440287\n",
      "Cost after iteration 9100: 0.440285\n",
      "Cost after iteration 9110: 0.440282\n",
      "Cost after iteration 9120: 0.440280\n",
      "Cost after iteration 9130: 0.440278\n",
      "Cost after iteration 9140: 0.440276\n",
      "Cost after iteration 9150: 0.440273\n",
      "Cost after iteration 9160: 0.440271\n",
      "Cost after iteration 9170: 0.440269\n",
      "Cost after iteration 9180: 0.440266\n",
      "Cost after iteration 9190: 0.440264\n",
      "Cost after iteration 9200: 0.440262\n",
      "Cost after iteration 9210: 0.440260\n",
      "Cost after iteration 9220: 0.440257\n",
      "Cost after iteration 9230: 0.440255\n",
      "Cost after iteration 9240: 0.440253\n",
      "Cost after iteration 9250: 0.440251\n",
      "Cost after iteration 9260: 0.440248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 9270: 0.440246\n",
      "Cost after iteration 9280: 0.440244\n",
      "Cost after iteration 9290: 0.440242\n",
      "Cost after iteration 9300: 0.440240\n",
      "Cost after iteration 9310: 0.440238\n",
      "Cost after iteration 9320: 0.440235\n",
      "Cost after iteration 9330: 0.440233\n",
      "Cost after iteration 9340: 0.440231\n",
      "Cost after iteration 9350: 0.440229\n",
      "Cost after iteration 9360: 0.440227\n",
      "Cost after iteration 9370: 0.440225\n",
      "Cost after iteration 9380: 0.440222\n",
      "Cost after iteration 9390: 0.440220\n",
      "Cost after iteration 9400: 0.440218\n",
      "Cost after iteration 9410: 0.440216\n",
      "Cost after iteration 9420: 0.440214\n",
      "Cost after iteration 9430: 0.440212\n",
      "Cost after iteration 9440: 0.440210\n",
      "Cost after iteration 9450: 0.440208\n",
      "Cost after iteration 9460: 0.440206\n",
      "Cost after iteration 9470: 0.440204\n",
      "Cost after iteration 9480: 0.440202\n",
      "Cost after iteration 9490: 0.440199\n",
      "Cost after iteration 9500: 0.440197\n",
      "Cost after iteration 9510: 0.440195\n",
      "Cost after iteration 9520: 0.440193\n",
      "Cost after iteration 9530: 0.440191\n",
      "Cost after iteration 9540: 0.440189\n",
      "Cost after iteration 9550: 0.440187\n",
      "Cost after iteration 9560: 0.440185\n",
      "Cost after iteration 9570: 0.440183\n",
      "Cost after iteration 9580: 0.440181\n",
      "Cost after iteration 9590: 0.440179\n",
      "Cost after iteration 9600: 0.440177\n",
      "Cost after iteration 9610: 0.440175\n",
      "Cost after iteration 9620: 0.440173\n",
      "Cost after iteration 9630: 0.440171\n",
      "Cost after iteration 9640: 0.440169\n",
      "Cost after iteration 9650: 0.440167\n",
      "Cost after iteration 9660: 0.440165\n",
      "Cost after iteration 9670: 0.440163\n",
      "Cost after iteration 9680: 0.440161\n",
      "Cost after iteration 9690: 0.440160\n",
      "Cost after iteration 9700: 0.440158\n",
      "Cost after iteration 9710: 0.440156\n",
      "Cost after iteration 9720: 0.440154\n",
      "Cost after iteration 9730: 0.440152\n",
      "Cost after iteration 9740: 0.440150\n",
      "Cost after iteration 9750: 0.440148\n",
      "Cost after iteration 9760: 0.440146\n",
      "Cost after iteration 9770: 0.440144\n",
      "Cost after iteration 9780: 0.440142\n",
      "Cost after iteration 9790: 0.440140\n",
      "Cost after iteration 9800: 0.440139\n",
      "Cost after iteration 9810: 0.440137\n",
      "Cost after iteration 9820: 0.440135\n",
      "Cost after iteration 9830: 0.440133\n",
      "Cost after iteration 9840: 0.440131\n",
      "Cost after iteration 9850: 0.440129\n",
      "Cost after iteration 9860: 0.440127\n",
      "Cost after iteration 9870: 0.440126\n",
      "Cost after iteration 9880: 0.440124\n",
      "Cost after iteration 9890: 0.440122\n",
      "Cost after iteration 9900: 0.440120\n",
      "Cost after iteration 9910: 0.440118\n",
      "Cost after iteration 9920: 0.440116\n",
      "Cost after iteration 9930: 0.440115\n",
      "Cost after iteration 9940: 0.440113\n",
      "Cost after iteration 9950: 0.440111\n",
      "Cost after iteration 9960: 0.440109\n",
      "Cost after iteration 9970: 0.440107\n",
      "Cost after iteration 9980: 0.440106\n",
      "Cost after iteration 9990: 0.440104\n",
      "train accuracy: 80.58361391694726 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_X, train_Y, test_X, num_iterations = 10000, learning_rate = 0.005, print_cost = True)\n",
    "predictions = d[\"Y_predict_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.reshape(-1,)\n",
    "result = pd.DataFrame({\"PassengerId\": test_data['PassengerId'].as_matrix(),\n",
    "                       \"Survived\": predictions})\n",
    "result['Survived'] = result['Survived'].astype(int)\n",
    "result.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 2 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Survived       418 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 6.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_Y = pd.read_csv(\"predictions.csv\")\n",
    "test_Y.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用sklearn的learning_curve得到training_score和cv_score，使用matplotlib画出learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1, \n",
    "                        train_sizes=np.linspace(.05, 1., 20), verbose=0, plot=True):\n",
    "    \"\"\"\n",
    "    画出data在某模型上的learning curve.\n",
    "    参数解释\n",
    "    ----------\n",
    "    estimator : 你用的分类器。\n",
    "    title : 表格的标题。\n",
    "    X : 输入的feature，numpy类型\n",
    "    y : 输入的target vector\n",
    "    ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点\n",
    "    cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)\n",
    "    n_jobs : 并行的的任务数(默认1)\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, verbose=verbose)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.title(title)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.xlabel(u\"train labels\")\n",
    "        plt.ylabel(u\"scores\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid()\n",
    "\n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                         alpha=0.1, color=\"b\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                         alpha=0.1, color=\"r\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=u\"train scores\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=u\"cv scores\")\n",
    "\n",
    "        plt.legend(loc=\"best\")\n",
    "\n",
    "        plt.draw()\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "\n",
    "    midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2\n",
    "    diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])\n",
    "    return midpoint, diff\n",
    "\n",
    "#plot_learning_curve(clf, u\"学习曲线\", X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看学习曲线\n",
    "estimator = GaussianNB()\n",
    "plot_learning_curve(estimator, u\"learning_rate\", train_X.T, train_Y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 Sklearn 练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression\n",
    "参数\n",
    "--- \n",
    "    penalty：使用指定正则化项（默认：l2）\n",
    "    dual: n_samples > n_features取False（默认）\n",
    "    C：正则化强度的反，值越小正则化强度越大\n",
    "    n_jobs: 指定线程数\n",
    "    random_state：随机数生成器\n",
    "    fit_intercept: 是否需要常量\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.80359147025813693"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn 的　逻辑回归\n",
    "model = LogisticRegression(penalty = 'l2', C = 1.0, tol = 0.0001)\n",
    "model.fit(train_X.T, train_Y.T)\n",
    "baest_params = model.get_params()\n",
    "# 模型　分数预测\n",
    "model.score(train_X.T, train_Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.7877095   0.81005587  0.79213483  0.78089888  0.8079096 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# 交叉验证\n",
    "print(cross_val_score(model, train_X.T, train_Y.T, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_X.T)\n",
    "predictions = predictions.reshape(-1,)\n",
    "result = pd.DataFrame({\"PassengerId\": test_data['PassengerId'].as_matrix(),\n",
    "                       \"Survived\": predictions})\n",
    "result['Survived'] = result['Survived'].astype(int)\n",
    "result.to_csv(\"predictions_sklearn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 投票逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxl/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "bag_model = LogisticRegression(penalty = 'l2', C = 1.0, tol = 0.0001)\n",
    "bagging_clf = BaggingRegressor(bag_model, n_estimators=20, max_samples=0.8, max_features=1.0, bootstrap=True,\n",
    "                               bootstrap_features=False, n_jobs=-1)\n",
    "bagging_clf.fit(train_X.T, train_Y.T)\n",
    "predictions = bagging_clf.predict(test_Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_X.T)\n",
    "predictions = predictions.reshape(-1,)\n",
    "result = pd.DataFrame({\"PassengerId\": test_data['PassengerId'].as_matrix(),\n",
    "                       \"Survived\": predictions})\n",
    "result['Survived'] = result['Survived'].astype(int)\n",
    "result.to_csv(\"predictions_bagging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 2 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Survived       418 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 6.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_Y = pd.read_csv(\"predictions_bagging.csv\")\n",
    "test_Y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
